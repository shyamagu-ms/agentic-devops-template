# テスト設計ガイドライン (単体・結合・性能・E2Eテスト)

各テストについて、設計書では目的・範囲と実施手順、想定するテストケース例を示します。テストコード自体の詳細は実装者に委ねますが、使用するツールやライブラリ、テスト観点は明確に定義します。

## 単体テスト (Unit Testing)

 - **目的**: 個々の関数やクラス、コンポーネントが期待どおり動作することを検証します。バグを早期に発見しやすくし、後工程での不具合混入を防ぐのが目的です。

 - **対象範囲**: フロントエンドでは各Svelteコンポーネントやユーティリティ関数、バックエンドでは各コントローラ関数やビジネスロジック単位です。副作用を持たない純粋関数はもちろん、外部依存がある場合はモックを使用して切り離してテストします。

 - **ツール**: テストフレームワークとしてVitest（Viteベースの高速テストランナー）やJestを用います。SvelteKitプロジェクト作成時にVitestを選択していれば、.spec/.testファイルをsrc内に配置するだけで実行可能です。バックエンドも同じフレームワークでテストできます。必要に応じてts-nodeやesm設定を行い、TypeScriptやESMモジュールのコードをテストできるようにします。

 - **実施方法**: 各ユニットについて、代表的な正常ケースと境界値、異常系のケースを洗い出します。例えば「入力xに対し期待する出力yを返す」「nullを渡した場合は例外を投げる」等です。モックライブラリ（例: Sinon）を用いて、外部のAPI呼び出しやデータベースアクセス部分はダミー化し、純粋にロジックのみを検証します。テストは開発者が機能実装と並行して随時実行できるよう、npm testで全テストが数十秒以内に終わる程度の粒度に保つことが望ましいです。

  - **設計書への記載**: 使用するテストフレームワーク名、テスト配置場所、命名規則（例: <関数名>_spec.ts）を示します。また、モック方針（どの外部依存をモックするか）やカバレッジ目標（例: ステートメントカバレッジ80%以上）など品質目標もあれば記載します。テストケース一覧表として、主要な単体テストケースとその期待結果をいくつか例示すると、実装者の指針になります。

## 結合テスト (Integration Testing)

 - **目的**: システム内の複数コンポーネントが連携して正しく動作することを検証します。モジュール間インタフェースの不整合や、外部システムとのやりとりの不備を発見するのが狙いです。

 - **対象範囲**: 例えばバックエンドAPIであれば、コントローラからDBアクセスまで含めた一連の処理を実環境に近い形でテストします。フロントエンドとバックエンドを同時に起動し、実際にHTTP経由で通信させるテストや、REST APIと外部サービス(API)との連携部分も対象です。

 - **ツール**: バックエンドAPIの結合テストにはSupertest（Node上でExpressアプリにリクエストをシミュレートするライブラリ）を利用し、実サーバーを立ち上げずにエンドポイント検証が可能です。またデータベースについては、ローカルにAzure SQL Edgeやエミュレータを立てるか、もしくはテスト用にAzure上にStagingのDBを用意し、そこで読み書きテストを行います（テスト後にロールバックするか、毎回初期化スクリプトを実行）。フロントエンドからAPIまで通す場合は、PlaywrightなどE2Eツールを利用するか、あるいはフロントエンドコードから直接HTTPクライアントを使ってAPIを呼ぶテストをJest等で書く方法もあります。

 - **実施方法**: まず各結合ポイントごとにテストシナリオを作成します。例：「ユーザー登録APIを呼び出すとDBにユーザーレコードが追加される」「外部の決済APIをモックしつつ注文フローを実行し、最終的に在庫と注文履歴が正しく更新される」などです。結合テストでは外部システムとの境界は可能な限り実際に呼び出しますが、テスト環境で呼べない場合はモックサービスを立てて対応します。各シナリオについて、期待するデータの流れと結果をアサートします（例: APIレスポンス内容およびDB上の該当レコードのフィールド値を検証）。

 - **環境とデータ**: テスト用の環境構築が必要です。CI上で結合テストを走らせる場合、GitHub ActionsやAzure Pipelinesでコンテナ化したサービスを起動し、シードデータ投入後にテストを実行、という流れを整備します。Azure SQLなどのPaaSをテストに使う場合は、事前にテスト用リソースをデプロイしておき、接続文字列をCIに設定します。テストデータは単体テスト同様、事前にDBに投入するか、テストケース内で作成します。終了時にはデータのクリーンアップ（削除）または次回のため毎回新規の隔離されたデータセットを使う工夫をします。

 - **設計書への記載**: どの結合部分を重視してテストするかを列挙します。例えば「API群とデータベースの結合テスト」「フロントエンドとバックエンド間の通信テスト」など章立てし、それぞれのテスト観点を記載します。また使用するモックサービスやテストダブルの方針（どこの結合を実物で試し、どこを疑似化するか）を明示します。これにより、結合テスト実装者がテスト環境を誤りなく構築できるようにします。

## パフォーマンステスト (性能テスト)

 - **目的**: システムが所定のパフォーマンス要件（応答時間、スループットなど）を満たすことを確認し、ボトルネックとなる箇所を特定します。特に負荷が高い状況での挙動（タイムアウトやメモリ不足等）が許容範囲内かを検証します。

 - **体制**: 性能テストは開発チームとインフラ運用チームが協力して行います。事前に非機能要件として「同時ユーザー◯人時に平均応答◯秒以内」等の目標値を設定し、テスト計画に落とし込みます。

 - **ツール**: 負荷シミュレーションにはApache JMeterを使用します。JMeterでシナリオ（テストプラン）を作成し、スレッドグループで仮想ユーザの挙動を記述します。例えば、ログイン→検索→詳細閲覧→ログアウトといった一連の操作をスクリプト化し、これを同時に何ユーザで実行するかを指定します。シナリオ作成時はThink Time（ユーザの待ち時間）も適切に入れ、実利用に近いパターンにします。

 - **実施方法**: 性能テスト環境（本番同等の構成が望ましいが、難しければ縮小版）に対し、徐々に負荷を高めるテストを実施します。例えば1分おきに同時ユーザ数を10→50→100人と段階的に上げ、それぞれ5分間維持するといった負荷プロファイルを設定します。JMeterはコマンドラインモードで実行し、Throughput(スループット/sec)、Latency(応答時間)、エラー率などのメトリクスを収集します。可能であればAzure MonitorやApplication Insightsとも連携し、テスト中のサーバCPU・メモリ使用率、SQLクエリ待機時間など内部指標も記録します。

 - **評価と対策**: テスト結果を分析し、目標を下回る箇所があれば原因を調査します。例えば応答時間が許容を超えた場合、そのときのスレッドダンプやSQL実行プランを確認し、ボトルネック（CPU限界、I/O競合、ロック競合など）を特定します。その結果に基づき、コードのチューニング（アルゴリズム改善、キャッシュ導入等）やインフラ調整（DBインデックス追加、プランのスケールアップ等）を行います。これら対策内容も設計書にフィードバックし、「性能上注意すべきクエリ」「改善策一覧」などを開発ナレッジとして残します。

 - **継続的テスト**: 性能テストは初回だけでなく、主要な変更のたびに自動実行できるのが望ましいです。CI/CDパイプラインにJMeterシナリオを組み込んだり、Azure Load Testingなどサービスを利用して繰り返し計測する仕組みを整えます。設計書では、どのタイミングで性能テストを実施するかや責任者を明確にし、継続的な性能監視計画として記載します。

## エンドツーエンドテスト (E2Eテスト, UIテスト)

 - **目的**: システム全体を通した動作を、ユーザ視点で検証します。UI上の操作からバックエンド、データベース更新まで含め、一連の業務シナリオが問題なく完遂できることを確認します。E2Eテストにより、システム統合後の最終品質を保証します。
対象範囲: 実際のユーザインタフェース（Webブラウザ）を操作するところから始まり、レンダリング結果、各種API呼び出し、副作用（DBの変更や他システムとの連携）まで含めてテストします。典型的には「◯◯画面で入力→送信→△△画面に結果表示」のようなユーザシナリオ単位でケースを作成します。

 - **ツール**: Playwright または Selenium を採用します。SvelteKitプロジェクトでは初期設定でPlaywrightを組み込めるため、これを利用するとよいでしょう。Playwrightはブラウザを自動操作してDOM要素を検証でき、複数ブラウザ（Chromium, WebKit, Firefox）でのテストも容易です。テストコードはTypeScript/JavaScriptで記述し、わかりやすいシナリオ形式で書けます（例: 「ページに移動→ログイン情報入力→ボタンクリック→結果を期待値と比較」）。

 - **シナリオ作成**: まず主要なユーザストーリーを洗い出し、それぞれをE2Eテストケースにします。例: ECサイトなら「商品検索してカート投入し購入完了まで」「マイページで登録情報変更してログアウトまで」などです。各ステップで確認すべき事象（画面遷移、表示メッセージ、バックエンドへのリクエスト結果）を明確にします。特に画面上の要素は、PlaywrightではIDやテキストで指定できるので、開発時にテストしやすいよう各重要要素にdata-test-id属性を仕込むなどの工夫も取り入れます。

 - **実施方法**: E2Eテストは本番に近い環境で行います。CI上で実施する場合、デプロイ後にテスト用のブラウザをヘッドレスモードで起動しテストを走らせます。検証項目としては、ページ表示の正否、期待するデータの表示（例: 登録したユーザ名がプロフィールページに表示される）、メール送信など非同期処理の完了確認、さらにコンソールエラーが出ていないか等もチェックします。Playwrightは自動的に失敗時のスクリーンショットやトレースを保存できるため、トラブルシューティングに活用します。

 - **設計書への記載**: シナリオ一覧を表形式で記載し、「前提状態」「操作」「期待結果」を整理します。例えば「【前提】ユーザーが存在【操作】ログイン画面でID/PW入力しログインボタン押下【期待結果】ダッシュボード画面にユーザー名が表示」といった形です。加えて、テスト環境で必要な初期データ（例えば特定のユーザアカウント）や外部サービスのダミー設定についても文書化します。E2Eテストは問題発生時の影響箇所特定が難しいため、失敗した場合の切り分け手順（どのログを確認し、単体機能テストとどう分離するか）などもナレッジとして残しておくとベターです。
